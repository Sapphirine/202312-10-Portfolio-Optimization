{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# ---- These codes are used to run historical tests\n",
    "# ----- Ekarat Tantawichet\n",
    "# ===========================================================\n",
    "\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# ---- IMPORT Packages\n",
    "# ===========================================================\n",
    "import finnhub\n",
    "finnhub_client = finnhub.Client(api_key=\"<<MASK>>\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.linalg import inv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from datetime import date, datetime, tzinfo, timedelta\n",
    "import datetime as dt\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "import pypfopt\n",
    "\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import plotting\n",
    "\n",
    "from pypfopt import EfficientFrontier\n",
    "\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import plotting\n",
    "\n",
    "from pypfopt import black_litterman, risk_models\n",
    "from pypfopt import BlackLittermanModel, plotting\n",
    "\n",
    "from pypfopt import EfficientFrontier, objective_functions\n",
    "\n",
    "import pickle \n",
    "\n",
    "# from google.cloud import storage\n",
    "# import os\n",
    "\n",
    "# --------------------------------------------------------------------------- \n",
    "from datetime import datetime, timedelta\n",
    "from textwrap import dedent\n",
    "import time\n",
    "\n",
    "\n",
    "####################################################\n",
    "# DEFINE PYTHON FUNCTIONS\n",
    "####################################################\n",
    "\n",
    "# -------- parameters ---------------------\n",
    "count = 0  # --- this is example of global variable in the def\n",
    "\n",
    "tickers_pull = ['XOM', 'HON', 'RTX', 'AMZN', 'PEP', 'UNH', 'JNJ', 'V', 'NVDA', 'AAPL', 'MSFT', 'GOOGL']\n",
    "\n",
    "# ======= initial parameters ============\n",
    "\n",
    "noofdays_test = 250\n",
    "# ======= lag 30 days for live runs ============\n",
    "noofdays_lag_live = 30\n",
    "\n",
    "# Get today's date\n",
    "today_real = date.today()\n",
    "\n",
    "today = today_real\n",
    "# print(\"Today is: \", today)\n",
    "\n",
    "# Yesterday date\n",
    "yesterday = today - timedelta(days = 1)\n",
    "# print(\"Yesterday was: \", yesterday)\n",
    "\n",
    "# --- market capitalization (December 2023, source Yahoo Finance)----\n",
    "mcaps = {'XOM': 398158000000,'HON': 132107000000,'RTX': 117750000000,'AMZN': 1508000000000,'PEP': 230729000000,'UNH': 502863000000,'JNJ': 373273000000, 'V': 527197000000,'NVDA': 1152000000000,'AAPL': 3004000000000,'MSFT': 2760000000000,'GOOGL': 1676000000000}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- regular functions ----------\n",
    "# ----- date format --------------\n",
    "def convert_date(x):\n",
    "    func_date = dt.datetime.fromtimestamp(x).strftime('%Y-%m-%d')\n",
    "    return func_date\n",
    "\n",
    "\n",
    "def convert_date_time(x):\n",
    "    func_date_time = dt.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return func_date_time\n",
    "\n",
    "# ----- definition of confidence level --------------\n",
    "# ------- confidence = 1.00 (high) if the prediction and sentiment agree -------\n",
    "# ------- confidence = 0.00 (low) if the prediction and sentiment disagree -----\n",
    "# ------- confidence = 0.50 (medium) if the sentiment is neutral -----\n",
    "\n",
    "def categorise(row):\n",
    "    if row['pred'] > 0 and row['sentiment'] ==1:\n",
    "        return 1\n",
    "    elif row['pred'] < 0 and row['sentiment'] ==-1:\n",
    "        return 1\n",
    "    elif row['pred'] > 0 and row['sentiment'] ==-1:\n",
    "        return 0\n",
    "    elif row['pred'] < 0 and row['sentiment'] ==1:\n",
    "        return 0\n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3a8c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ dates to be tested -----------------\n",
    "date_list_x2 = [['2023-10-4', '10/3/2023', '10/2/2023'],\n",
    "['2023-10-5', '10/4/2023', '10/3/2023'],\n",
    "['2023-10-6', '10/5/2023', '10/4/2023'],\n",
    "['2023-10-9', '10/6/2023', '10/5/2023'],\n",
    "['2023-10-10', '10/9/2023', '10/6/2023'],\n",
    "['2023-10-11', '10/10/2023', '10/9/2023'],\n",
    "['2023-10-12', '10/11/2023', '10/10/2023'],\n",
    "['2023-10-13', '10/12/2023', '10/11/2023'],\n",
    "['2023-10-16', '10/13/2023', '10/12/2023'],\n",
    "['2023-10-17', '10/16/2023', '10/13/2023'],\n",
    "['2023-10-18', '10/17/2023', '10/16/2023'],\n",
    "['2023-10-19', '10/18/2023', '10/17/2023'],\n",
    "['2023-10-20', '10/19/2023', '10/18/2023'],\n",
    "['2023-10-23', '10/20/2023', '10/19/2023'],\n",
    "['2023-10-24', '10/23/2023', '10/20/2023'],\n",
    "['2023-10-25', '10/24/2023', '10/23/2023'],\n",
    "['2023-10-26', '10/25/2023', '10/24/2023'],\n",
    "['2023-10-27', '10/26/2023', '10/25/2023'],\n",
    "['2023-10-30', '10/27/2023', '10/26/2023'],\n",
    "['2023-10-31', '10/30/2023', '10/27/2023'],\n",
    "['2023-11-1', '10/31/2023', '10/30/2023'],\n",
    "['2023-11-2', '11/1/2023', '10/31/2023'],\n",
    "['2023-11-3', '11/2/2023', '11/1/2023'],\n",
    "['2023-11-6', '11/3/2023', '11/2/2023'],\n",
    "['2023-11-7', '11/6/2023', '11/3/2023'],\n",
    "['2023-11-8', '11/7/2023', '11/6/2023'],\n",
    "['2023-11-9', '11/8/2023', '11/7/2023'],\n",
    "['2023-11-10', '11/9/2023', '11/8/2023'],\n",
    "['2023-11-13', '11/10/2023', '11/9/2023'],\n",
    "['2023-11-14', '11/13/2023', '11/10/2023'],\n",
    "['2023-11-15', '11/14/2023', '11/13/2023'],\n",
    "['2023-11-16', '11/15/2023', '11/14/2023'],\n",
    "['2023-11-17', '11/16/2023', '11/15/2023'],\n",
    "['2023-11-20', '11/17/2023', '11/16/2023'],\n",
    "['2023-11-21', '11/20/2023', '11/17/2023'],\n",
    "['2023-11-22', '11/21/2023', '11/20/2023'],\n",
    "['2023-11-24', '11/22/2023', '11/21/2023'],\n",
    "['2023-11-27', '11/24/2023', '11/22/2023'],\n",
    "['2023-11-28', '11/27/2023', '11/24/2023'],\n",
    "['2023-11-29', '11/28/2023', '11/27/2023'],\n",
    "['2023-11-30', '11/29/2023', '11/28/2023'],\n",
    "['2023-12-1', '11/30/2023', '11/29/2023'],\n",
    "['2023-12-4', '12/1/2023', '11/30/2023'],\n",
    "['2023-12-5', '12/4/2023', '12/1/2023'],\n",
    "['2023-12-6', '12/5/2023', '12/4/2023'],\n",
    "['2023-12-7', '12/6/2023', '12/5/2023'],\n",
    "['2023-12-8', '12/7/2023', '12/6/2023'],\n",
    "['2023-12-11', '12/8/2023', '12/7/2023'],\n",
    "['2023-12-12', '12/11/2023', '12/8/2023'],\n",
    "['2023-12-13', '12/12/2023', '12/11/2023'],\n",
    "['2023-12-14', '12/13/2023', '12/12/2023']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070c966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- step 1 (tp_1): pull new data and calculate returns etc.\n",
    "def step1_pull_stock_data(pull_end_date):\n",
    "    tickers = tickers_pull\n",
    "    # ohlc = yf.download(tickers, period=\"60mo\") # 60 months start=\"12/11/18\"\n",
    "    ohlc = yf.download(tickers, start=\"2018-12-11\", end=pull_end_date)\n",
    "    prices = ohlc[\"Adj Close\"]\n",
    "    # prices\n",
    "    # market_prices_BL_raw = yf.download(\"SPY\", period=\"60mo\")[\"Adj Close\"]\n",
    "    returns_full = prices.pct_change()\n",
    "    returns= returns_full.dropna()\n",
    "    # returns\n",
    "    # returns_full =  returns_full.reset_index()\n",
    "    # df = returns        \n",
    "    market_prices_BL_pull = yf.download(\"SPY\", period=\"60mo\")[\"Adj Close\"]\n",
    "    market_prices_BL_raw = market_prices_BL_pull\n",
    "    returns.to_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/df.csv', index ='Date')\n",
    "    prices.to_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/prices.csv', index ='Date')\n",
    "    market_prices_BL_raw.to_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/market_prices_BL_raw.csv', index='Date') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86413dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- step 2 (tp_2): fit XGBoost and then make prediction based on new data and calculate returns etc. -----\n",
    "def step2_ml_xg():\n",
    "    # ----- XGBoost Parameters ------------\n",
    "    xg_max_depth=50\n",
    "    xg_learning_rate=0.8\n",
    "    xg_reg_lambda=8\n",
    "    xg_subsample=0.4\n",
    "    xg_grow_policy=\"lossguide\"    \n",
    "    # ----- ONE DAY PREDICTION:: LIVE  ------------    \n",
    "    df = pd.read_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/df.csv', index_col='Date')    \n",
    "    # training_prices_x = {}\n",
    "    # training_prices_y = {}\n",
    "    live_prices_x = {}\n",
    "    live_prices_y = {}\n",
    "    live_pred_x = {}\n",
    "    pred_live_30 = df.tail(noofdays_lag_live)\n",
    "    # live_pred_x = {}\n",
    "    for col in df.columns:\n",
    "        company_live = df[col].to_numpy()\n",
    "        company_live_30 = pred_live_30[col].to_numpy()\n",
    "        company_live_x = [company_live[i:i+15] for i in range(len(company_live)-15)]\n",
    "        company_live_y = [company_live[i+1] for i in range(14,len(company_live)-1)]\n",
    "        # company_live_pred_x = [company_live[i:i+30+1] for i in range(len(company_live)-30+1)]\n",
    "        company_live_pred_x= [company_live_30[i:i+15] for i in range(len(company_live)-15)]\n",
    "        live_prices_x[col] = company_live_x\n",
    "        live_prices_y[col] = company_live_y\n",
    "        live_pred_x[col] = [company_live_pred_x[0]]\n",
    "        # live_pred_x\n",
    "    # ----- ONE DAY PREDICTION LIVE ------------\n",
    "    next_day_preds ={}\n",
    "    for col in df.columns:\n",
    "        bst = xgb.XGBRegressor(max_depth=xg_max_depth, learning_rate=xg_learning_rate,\n",
    "                               reg_lambda=xg_reg_lambda, subsample=xg_subsample, grow_policy=xg_grow_policy)\n",
    "        bst = bst.fit(live_prices_x[col],live_prices_y[col])\n",
    "        next_day_preds[col] = bst.predict(live_pred_x[col])\n",
    "    # next_day_preds    \n",
    "    next_day_preds_df = pd.DataFrame.from_dict(next_day_preds)\n",
    "    next_day_preds_df.index = ['pred']*1\n",
    "    # next_day_preds_df\n",
    "    next_day_preds_df.to_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/next_day_preds_df.csv', index=True)\n",
    "    with open('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/saved_dictionary.pkl', 'wb') as f_next_day_preds:\n",
    "        pickle.dump(next_day_preds, f_next_day_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca9caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- step 3 (tp_3): pull news feed and estimate sentiment for each news -----\n",
    "def step3_news_feed(yesterday_date):    \n",
    "    stock_list = tickers_pull\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    model_finbert = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "    sentiment_list =[]\n",
    "    for stock in stock_list:\n",
    "\n",
    "        news_series = finnhub_client.company_news(stock, _from=yesterday_date, to=yesterday_date)\n",
    "        if news_series == []:\n",
    "            print(\"stock news \"+stock+\" is empty\")\n",
    "            sentiment_stock  = 0\n",
    "        else:\n",
    "            news_series_df = pd.DataFrame.from_dict(news_series)\n",
    "            result_date = []\n",
    "            result_news = []\n",
    "            for index, row in news_series_df.iterrows():\n",
    "                result_news.append(row['headline'])\n",
    "                result_date.append(convert_date(row['datetime']))\n",
    "            \n",
    "            result_date\n",
    "            inputs = tokenizer(result_news, padding = True, truncation = True, return_tensors='pt')\n",
    "            outputs = model_finbert(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            pred_arr = predictions.detach().cpu().numpy()\n",
    "            result_date_df = pd.DataFrame({'date' : result_date}, columns=['date'])\n",
    "\n",
    "            result_news_df = pd.DataFrame(data = pred_arr, columns = [\"Positive\", \"Negative\", \"Neutral\"])\n",
    "\n",
    "            result_combined = pd.concat([result_date_df, result_news_df], axis=1)\n",
    "\n",
    "            result_daily_nodate = result_combined.drop(columns=['date'])\n",
    "            result_daily = result_daily_nodate.mean()\n",
    "\n",
    "            conditions = [(result_daily['Positive'] > result_daily['Negative']) & (result_daily['Positive'] > result_daily['Neutral']),\n",
    "            (result_daily['Negative'] > result_daily['Positive']) & (result_daily['Negative'] > result_daily['Neutral']),\n",
    "            (result_daily['Neutral'] > result_daily['Positive']) & (result_daily['Neutral'] > result_daily['Positive'])]\n",
    "            sentiment_values = [1, -1, 0]\n",
    "            result_daily[stock] = np.select(conditions, sentiment_values)\n",
    "            sentiment_stock  = np.ndarray.item(np.array([result_daily[stock]]))\n",
    "        sentiment_list.append(sentiment_stock)\n",
    "\n",
    "    sentiment_list_row =[sentiment_list]\n",
    "    sentiment_daily = pd.DataFrame(sentiment_list_row, columns=stock_list, index=['sentiment']*1)\n",
    "    print(tickers_pull)\n",
    "    print(sentiment_list)\n",
    "    sentiment_daily.to_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/sentiment_daily.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- step 4 (tp_4): run Black Litterman using pypfopt API to get stock holding (portfolio) recommendation -----    \n",
    "# -------- and then generate previous days performance (portfolio return, standard deviation, and Sharpe-ratio ------  \n",
    "\n",
    "def step4_bl_weight():  \n",
    "\n",
    "    prices_BL = pd.read_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/prices.csv', index_col='Date')    \n",
    "    df_market_csv_read = pd.read_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/market_prices_BL_raw.csv', index_col='Date')\n",
    "    market_prices_BL_raw=df_market_csv_read[df_market_csv_read.columns[0]]    \n",
    "    next_day_preds_df = pd.read_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/next_day_preds_df.csv', index_col=['Unnamed: 0'])\n",
    "    sentiment_daily = pd.read_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/sentiment_daily.csv', index_col=['Unnamed: 0'])    \n",
    "    with open('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/saved_dictionary.pkl', 'rb') as f_next_day_preds: next_day_preds = pickle.load(f_next_day_preds)    \n",
    "\n",
    "    next_day_preds_df_T = next_day_preds_df.T\n",
    "\n",
    "    sentiment_daily_T = sentiment_daily.T\n",
    "\n",
    "    combine_pred_sentiment = next_day_preds_df_T.join(sentiment_daily_T)\n",
    "    # combine_pred_sentiment\n",
    "    combine_pred_sentiment['confidence'] = combine_pred_sentiment.apply(lambda row: categorise(row), axis=1)\n",
    "    confidence = combine_pred_sentiment['confidence'].T\n",
    "\n",
    "    market_prices_BL = market_prices_BL_raw\n",
    "    viewdict_ML2_GBReg = next_day_preds\n",
    "    confidences_ML2_GBReg = confidence\n",
    "    S_BL = risk_models.CovarianceShrinkage(prices_BL).ledoit_wolf()\n",
    "    delta = black_litterman.market_implied_risk_aversion(market_prices_BL)\n",
    "    # delta\n",
    "    market_prior = black_litterman.market_implied_prior_returns(mcaps, delta, S_BL)\n",
    "    # market_prior\n",
    "    bl_ML2_GBReg = BlackLittermanModel(S_BL, pi=market_prior, absolute_views=viewdict_ML2_GBReg, omega=\"idzorek\", view_confidences=confidences_ML2_GBReg)\n",
    "    bl = bl_ML2_GBReg\n",
    "    omega_ML2_GBReg = bl.omega\n",
    "    # We are using the shortcut to automatically compute market-implied prior\n",
    "    bl_ML2_GBReg = BlackLittermanModel(S_BL, pi=\"market\", market_caps=mcaps, risk_aversion=delta,\n",
    "    absolute_views=viewdict_ML2_GBReg, omega=omega_ML2_GBReg)\n",
    "    # Posterior estimate of returns\n",
    "    ret_bl_ML2_GBReg = bl.bl_returns()\n",
    "    # ret_bl_ML2_GBReg\n",
    "    rets_df_ML2_GBReg = pd.DataFrame([market_prior, ret_bl_ML2_GBReg, pd.Series(viewdict_ML2_GBReg)],\n",
    "     index=[\"Prior\", \"Posterior\", \"Views\"]).T\n",
    "    # rets_df_ML2_GBReg\n",
    "    # rets_df_ML2_GBReg.plot.bar(figsize=(12,8));\n",
    "    S_bl_ML2_GBReg = bl_ML2_GBReg.bl_cov()\n",
    "    # plotting.plot_covariance(S_bl_ML2_GBReg);\n",
    "    ef = EfficientFrontier(ret_bl_ML2_GBReg, S_bl_ML2_GBReg)\n",
    "    ef.add_objective(objective_functions.L2_reg)\n",
    "    ef.max_sharpe()\n",
    "    weights_ML2_GBReg = ef.clean_weights()\n",
    "    # weights_ML2_GBReg\n",
    "    weights_ML2_GBReg_df = pd.DataFrame(weights_ML2_GBReg, index=[today])\n",
    "    # print(weights_ML2_GBReg_df)\n",
    "    # weights_ML2_GBReg_df_out = weights_ML2_GBReg_df_out.append(weights_ML2_GBReg_df)\n",
    "    weights_ML2_GBReg_df.to_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/weights_ML2_GBReg_df.csv')\n",
    "    #   ---------------- ADDED FOR REPORT and STACKING UP -----------------------------\n",
    "    w_hist = pd.read_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/w_hist.csv', index_col='Date')\n",
    "    df = pd.read_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/df.csv', index_col='Date')\n",
    "    df_ret_calc = df.add_prefix('ret_')\n",
    "    df_ret_calc.index = pd.to_datetime(df_ret_calc.index).strftime('%m/%d/%Y')\n",
    "    # df_ret_calc\n",
    "    w_hist_calc = w_hist.add_prefix('w_')\n",
    "    w_hist_calc.index = pd.to_datetime(w_hist_calc.index).strftime('%m/%d/%Y')\n",
    "    port = w_hist_calc.merge(df_ret_calc, how='inner', left_index=True, right_index=True)\n",
    "    stock_list = tickers_pull\n",
    "    for stock_str in stock_list:\n",
    "        port['x_'+stock_str] = port['w_'+stock_str]*port['ret_'+stock_str]\n",
    "        port = port.drop('w_'+stock_str, axis=1)\n",
    "        port = port.drop('ret_'+stock_str, axis=1)\n",
    "    port_wavg_ret = port.sum(axis=1)\n",
    "    sharpe_ratio_port = port_wavg_ret.mean()/port_wavg_ret.std()\n",
    "    rm_ret = market_prices_BL_raw.pct_change()\n",
    "    rm_ret = rm_ret.dropna()\n",
    "    rm_ret.index = pd.to_datetime(rm_ret.index).strftime('%m/%d/%Y')\n",
    "    rm = port_wavg_ret.to_frame().join(rm_ret.to_frame())\n",
    "    sharp_ratio_rm = rm['Adj Close'].mean()/rm['Adj Close'].std()\n",
    "    sharpe_summary_list = [[ port_wavg_ret.index.max(), sharpe_ratio_port, sharp_ratio_rm]]\n",
    "    sharpe_summary = pd.DataFrame(sharpe_summary_list, columns=['Date', 'Sharpe Ratio Portfolio (30 days)', 'Sharpe Ratio S&P (30 days)'])\n",
    "    sharpe_summary_csv = sharpe_summary.set_index('Date')\n",
    "    sharpe_summary_csv.to_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/sharpe_summary_csv.csv', index_label='Date')\n",
    "    return_summary_list = [[ port_wavg_ret.index.max(), port_wavg_ret.mean(), port_wavg_ret.std(), rm['Adj Close'].mean(), rm['Adj Close'].std()]]\n",
    "    return_summary = pd.DataFrame(return_summary_list, columns=['Date', 'Portfolio Avg Return (30 days)', 'Portfolio Return Std (30 days)', 'S&P Avg Return  (30 days)', 'S&P Return Std (30 days)'])\n",
    "    return_summary_csv = return_summary.set_index('Date')\n",
    "    return_summary_csv.to_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/return_summary_csv.csv', index_label='Date')\n",
    "    print(\"return_summary_csv\")\n",
    "    print(return_summary_csv)\n",
    "    # ---- finally we can stack up the weights\n",
    "    weights_ML2_GBReg_df.index.name='Date'\n",
    "    weights_ML2_GBReg_df.index = pd.to_datetime(weights_ML2_GBReg_df.index).strftime('%m/%d/%Y')\n",
    "    # w_hist_app = w_hist.append(weights_ML2_GBReg_df)\n",
    "    print(weights_ML2_GBReg_df)\n",
    "    w_hist_app = pd.concat([w_hist, weights_ML2_GBReg_df])\n",
    "    w_hist = w_hist_app.tail(30)\n",
    "    w_hist.to_csv('C:/Users/Ed/EECS6893_BigDataAnalytics_FALL_2023/proj_fall2023/data_bucket/manual_test_dec12/w_hist.csv', index_label='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba08ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ----- Run history -------------\n",
    "from datetime import datetime\n",
    "for day_str in date_list_x2:\n",
    "    yfin_end_date = datetime.strptime(day_str[0], '%Y-%m-%d').date()\n",
    "    today = datetime.strptime(day_str[1], '%m/%d/%Y').date()\n",
    "    yesterday_date = datetime.strptime(day_str[2], '%m/%d/%Y').date()\n",
    "    step1_pull_stock_data(yfin_end_date)\n",
    "    step2_ml_xg()\n",
    "    print(today)\n",
    "    step3_news_feed(yesterday_date)\n",
    "    step4_bl_weight()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
